{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Deo0-usWoBBx"
   },
   "source": [
    "# Exercises Sheet 3 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nimport random\\nimport matplotlib.pyplot as plt\\n\\nfield = np.zeros((10,10),dtype=int)\\n\\nprint(field[1][1])\\nprint(field)\\nprint(len(field[0]))'"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "field = np.zeros((10,10),dtype=int)\n",
    "\n",
    "print(field[1][1])\n",
    "print(field)\n",
    "print(len(field[0]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nimport random\\nimport matplotlib.pyplot as plt\\nimport time  # Optional delay for visualizing matrix updates\\n\\nclass Environment:\\n    def __init__(self):\\n        # Define the grid size and initialize the field matrix\\n        self.field = np.zeros((10, 10), dtype=str)  # 10x10 grid initialized as empty\\n        self.field[:] = \\' \\'  # Fill with empty spaces\\n        self.goal_state = [9, 9]  # Goal at bottom-right corner\\n        self.max_steps = 1000  # Maximum steps per episode\\n\\n    def transition(self, state, action):\\n        \"\"\"State transition function based on action (up, down, left, right).\"\"\"\\n        # Apply the action, updating the agent\\'s row or column position\\n        if action == \"up\" and state[0] > 0: # linhas\\n            state[0] -= 1 # linhas - 1\\n        elif action == \"down\" and state[0] < len(field[0]) - 1: # linhas\\n            state[0] += 1 # linhas + 1\\n        elif action == \"left\" and state[1] > 0:\\n            state[1] -= 1 # colunas - 1\\n        elif action == \"right\" and state[1] < self.field.shape[1] - 1:\\n            state[1] += 1 # colunas + 1\\n        return state\\n\\n    def reward(self, state):\\n        \"\"\"Reward function: 100 if goal is reached, otherwise 0.\"\"\"\\n        return 100 if state == self.goal_state else 0\\n\\n    def print_matrix(self, agent_state):\\n        \"\"\"Print the field with \\'O\\' for the agent and \\'X\\' for the goal.\"\"\"\\n        # Create a copy of the field to modify for printing\\n        display_field = self.field.copy()\\n        # Mark the agent\\'s and goal\\'s positions\\n        display_field[agent_state[0], agent_state[1]] = \\'O\\'\\n        display_field[self.goal_state[0], self.goal_state[1]] = \\'X\\'\\n        \\n        # Print each row of the matrix\\n        for row in display_field:\\n            print(\\' \\'.join(row))\\n        print(\"\\n\" + \"-\" * 20)  # Divider between prints\\n\\nclass Agent:\\n    def __init__(self, environment):\\n        self.env = environment\\n        self.current_state = [0, 0]  # Starting position at top-left corner\\n        self.actions = [\"up\", \"down\", \"left\", \"right\"]\\n\\n    def choose_random_action(self):\\n        \"\"\"Randomly select an action.\"\"\"\\n        return random.choice(self.actions)\\n\\n    def reset(self):\\n        \"\"\"Reset the agent to the starting position.\"\"\"\\n        self.current_state = [0, 0]\\n\\n    def run_episode(self):\\n        \"\"\"Run an episode until goal is reached or max steps are exceeded.\"\"\"\\n        total_reward = 0\\n        steps = 0\\n\\n        for i in range(self.env.max_steps):\\n            action = self.choose_random_action()\\n            new_state = self.env.transition(self.current_state, action)\\n            total_reward += self.env.reward(new_state)\\n            steps += 1\\n            \\n\\n            # Print the matrix every 100 iterations\\n            if i % 100 == 0:\\n                print(f\"Iteration {i}\")\\n                self.env.print_matrix(self.current_state)\\n\\n            # Check if goal state is reached\\n            if self.current_state == self.env.goal_state:\\n                print(\"Reached the goal!\")\\n                break\\n\\n        self.reset()\\n        return total_reward, steps\\n\\n# Run simulation for 30 episodes\\ndef simulate(agent, num_episodes=30):\\n    rewards, steps_list = [], []\\n\\n    for _ in range(num_episodes):\\n        reward, steps = agent.run_episode()\\n        rewards.append(reward)\\n        steps_list.append(steps)\\n\\n    print(f\"Reward: {rewards}\")\\n    print(f\"Steps: {steps_list}\")\\n\\n    return rewards, steps_list\\n\\n\\ndef plot_analyze(rewards, steps_list):\\n    \"\"\"Use to analyze results.\"\"\"\\n    # Calculate statistics\\n    avg_reward = np.mean(rewards)\\n    avg_steps = np.mean(steps_list)\\n    std_steps = np.std(steps_list)\\n\\n    # Plotting the results\\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\\n\\n    ax[0].boxplot(rewards, vert=True)\\n    ax[0].set_title(\"Rewards per Episode\")\\n    ax[0].set_ylabel(\"Reward\")\\n\\n    ax[1].boxplot(steps_list, vert=True)\\n    ax[1].set_title(\"Steps to Reach Goal\")\\n    ax[1].set_ylabel(\"Steps\")\\n\\n    plt.show()\\n\\n    print(f\"Average Reward per Episode: {avg_reward}\")\\n    print(f\"Average Steps to Goal: {avg_steps}\")\\n    print(f\"Standard Deviation of Steps: {std_steps}\")\\n\\n# Initialize environment and agent\\nenv = Environment()\\nagent = Agent(env)\\n\\n# Run the simulation\\nrewards, steps_list = simulate(agent)\\nplot_analyze(rewards, steps_list)\\n'"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time  # Optional delay for visualizing matrix updates\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        # Define the grid size and initialize the field matrix\n",
    "        self.field = np.zeros((10, 10), dtype=str)  # 10x10 grid initialized as empty\n",
    "        self.field[:] = ' '  # Fill with empty spaces\n",
    "        self.goal_state = [9, 9]  # Goal at bottom-right corner\n",
    "        self.max_steps = 1000  # Maximum steps per episode\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        \"\"\"State transition function based on action (up, down, left, right).\"\"\"\n",
    "        # Apply the action, updating the agent's row or column position\n",
    "        if action == \"up\" and state[0] > 0: # linhas\n",
    "            state[0] -= 1 # linhas - 1\n",
    "        elif action == \"down\" and state[0] < len(field[0]) - 1: # linhas\n",
    "            state[0] += 1 # linhas + 1\n",
    "        elif action == \"left\" and state[1] > 0:\n",
    "            state[1] -= 1 # colunas - 1\n",
    "        elif action == \"right\" and state[1] < self.field.shape[1] - 1:\n",
    "            state[1] += 1 # colunas + 1\n",
    "        return state\n",
    "\n",
    "    def reward(self, state):\n",
    "        \"\"\"Reward function: 100 if goal is reached, otherwise 0.\"\"\"\n",
    "        return 100 if state == self.goal_state else 0\n",
    "\n",
    "    def print_matrix(self, agent_state):\n",
    "        \"\"\"Print the field with 'O' for the agent and 'X' for the goal.\"\"\"\n",
    "        # Create a copy of the field to modify for printing\n",
    "        display_field = self.field.copy()\n",
    "        # Mark the agent's and goal's positions\n",
    "        display_field[agent_state[0], agent_state[1]] = 'O'\n",
    "        display_field[self.goal_state[0], self.goal_state[1]] = 'X'\n",
    "        \n",
    "        # Print each row of the matrix\n",
    "        for row in display_field:\n",
    "            print(' '.join(row))\n",
    "        print(\"\\n\" + \"-\" * 20)  # Divider between prints\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, environment):\n",
    "        self.env = environment\n",
    "        self.current_state = [0, 0]  # Starting position at top-left corner\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "    def choose_random_action(self):\n",
    "        \"\"\"Randomly select an action.\"\"\"\n",
    "        return random.choice(self.actions)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent to the starting position.\"\"\"\n",
    "        self.current_state = [0, 0]\n",
    "\n",
    "    def run_episode(self):\n",
    "        \"\"\"Run an episode until goal is reached or max steps are exceeded.\"\"\"\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        for i in range(self.env.max_steps):\n",
    "            action = self.choose_random_action()\n",
    "            new_state = self.env.transition(self.current_state, action)\n",
    "            total_reward += self.env.reward(new_state)\n",
    "            steps += 1\n",
    "            \n",
    "\n",
    "            # Print the matrix every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}\")\n",
    "                self.env.print_matrix(self.current_state)\n",
    "\n",
    "            # Check if goal state is reached\n",
    "            if self.current_state == self.env.goal_state:\n",
    "                print(\"Reached the goal!\")\n",
    "                break\n",
    "\n",
    "        self.reset()\n",
    "        return total_reward, steps\n",
    "\n",
    "# Run simulation for 30 episodes\n",
    "def simulate(agent, num_episodes=30):\n",
    "    rewards, steps_list = [], []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        reward, steps = agent.run_episode()\n",
    "        rewards.append(reward)\n",
    "        steps_list.append(steps)\n",
    "\n",
    "    print(f\"Reward: {rewards}\")\n",
    "    print(f\"Steps: {steps_list}\")\n",
    "\n",
    "    return rewards, steps_list\n",
    "\n",
    "\n",
    "def plot_analyze(rewards, steps_list):\n",
    "    \"\"\"Use to analyze results.\"\"\"\n",
    "    # Calculate statistics\n",
    "    avg_reward = np.mean(rewards)\n",
    "    avg_steps = np.mean(steps_list)\n",
    "    std_steps = np.std(steps_list)\n",
    "\n",
    "    # Plotting the results\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    ax[0].boxplot(rewards, vert=True)\n",
    "    ax[0].set_title(\"Rewards per Episode\")\n",
    "    ax[0].set_ylabel(\"Reward\")\n",
    "\n",
    "    ax[1].boxplot(steps_list, vert=True)\n",
    "    ax[1].set_title(\"Steps to Reach Goal\")\n",
    "    ax[1].set_ylabel(\"Steps\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Average Reward per Episode: {avg_reward}\")\n",
    "    print(f\"Average Steps to Goal: {avg_steps}\")\n",
    "    print(f\"Standard Deviation of Steps: {std_steps}\")\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = Environment()\n",
    "agent = Agent(env)\n",
    "\n",
    "# Run the simulation\n",
    "rewards, steps_list = simulate(agent)\n",
    "plot_analyze(rewards, steps_list)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "Reached the goal!\n",
      "OLAAAAAAAAAAAAAAAAAAADSADSJFHUBVNCJDHBGVVHUNDHERGFHBNDJEHFRUD\n",
      "[[[  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[  0.99         0.99         0.99         0.99      ]\n",
      "  [  0.99         0.99         0.99         0.99      ]\n",
      "  [  0.99         0.99         0.99         0.99      ]\n",
      "  [  0.99         0.99         0.99         0.99      ]\n",
      "  [  0.99         0.99         0.99         0.99      ]\n",
      "  [  0.99         0.99         0.99         0.99      ]\n",
      "  [  0.99         0.99         0.99         0.99      ]\n",
      "  [  0.99         0.99         0.99         0.99      ]\n",
      "  [  0.99         0.99         0.99         0.99      ]\n",
      "  [  0.99         0.99         0.99         0.99      ]]\n",
      "\n",
      " [[  1.98         1.98         1.98         1.98      ]\n",
      "  [  1.98         1.98         1.98         1.98      ]\n",
      "  [  1.98         1.98         1.98         1.98      ]\n",
      "  [  1.98         1.98         1.98         1.98      ]\n",
      "  [  1.98         1.98         1.98         1.98      ]\n",
      "  [  1.98         1.98         1.98         1.98      ]\n",
      "  [  1.98         1.98         1.98         1.98      ]\n",
      "  [  1.98         1.98         1.98         1.98      ]\n",
      "  [  1.98         1.98         1.98         1.98      ]\n",
      "  [  1.98         1.98         1.98         1.98      ]]\n",
      "\n",
      " [[  2.97         2.97         2.97         2.97      ]\n",
      "  [  2.97         2.97         2.97         2.97      ]\n",
      "  [  2.97         2.97         2.97         2.97      ]\n",
      "  [  2.97         2.97         2.97         2.97      ]\n",
      "  [  2.97         2.97         2.97         2.97      ]\n",
      "  [  2.97         2.97         2.97         2.97      ]\n",
      "  [  2.97         2.97         2.97         2.97      ]\n",
      "  [  2.97         2.97         2.97         2.97      ]\n",
      "  [  2.97         2.97         2.97         2.97      ]\n",
      "  [  2.97         2.97         2.97         2.97      ]]\n",
      "\n",
      " [[  3.96         3.96         3.96         3.96      ]\n",
      "  [  3.96         3.96         3.96         3.96      ]\n",
      "  [  3.96         3.96         3.96         3.96      ]\n",
      "  [  3.96         3.96         3.96         3.96      ]\n",
      "  [  3.96         3.96         3.96         3.96      ]\n",
      "  [  3.96         3.96         3.96         3.96      ]\n",
      "  [  3.96         3.96         3.96         3.96      ]\n",
      "  [  3.96         3.96         3.96         3.96      ]\n",
      "  [  3.96         3.96         3.96         3.96      ]\n",
      "  [  3.96         3.96         3.96         3.96      ]]\n",
      "\n",
      " [[  4.95         4.95         4.95         4.95      ]\n",
      "  [  4.95         4.95         4.95         4.95      ]\n",
      "  [  4.95         4.95         4.95         4.95      ]\n",
      "  [  4.95         4.95         4.95         4.95      ]\n",
      "  [  4.95         4.95         4.95         4.95      ]\n",
      "  [  4.95         4.95         4.95         4.95      ]\n",
      "  [  4.95         4.95         4.95         4.95      ]\n",
      "  [  4.95         4.95         4.95         4.95      ]\n",
      "  [  4.95         4.95         4.95         4.95      ]\n",
      "  [  4.95         4.95         4.95         4.95      ]]\n",
      "\n",
      " [[  5.94         5.94         5.94         5.94      ]\n",
      "  [  5.94         5.94         5.94         5.94      ]\n",
      "  [  5.94         5.94         5.94         5.94      ]\n",
      "  [  5.94         5.94         5.94         5.94      ]\n",
      "  [  5.94         5.94         5.94         5.94      ]\n",
      "  [  5.94         5.94         5.94         5.94      ]\n",
      "  [  5.94         5.94         5.94         5.94      ]\n",
      "  [  5.94         5.94         5.94         5.94      ]\n",
      "  [  5.94         5.94         5.94         5.94      ]\n",
      "  [  5.94         5.94         5.94         5.94      ]]\n",
      "\n",
      " [[  6.93         6.93         6.93         6.93      ]\n",
      "  [  6.93         6.93         6.93         6.93      ]\n",
      "  [  6.93         6.93         6.93         6.93      ]\n",
      "  [  6.93         6.93         6.93         6.93      ]\n",
      "  [  6.93         6.93         6.93         6.93      ]\n",
      "  [  6.93         6.93         6.93         6.93      ]\n",
      "  [  6.93         6.93         6.93         6.93      ]\n",
      "  [  6.93         6.93         6.93         6.93      ]\n",
      "  [  6.93         6.93         6.93         6.93      ]\n",
      "  [  6.93         6.93         6.93         6.93      ]]\n",
      "\n",
      " [[  7.92         7.92         7.92         7.92      ]\n",
      "  [  7.92         7.92         7.92         7.92      ]\n",
      "  [  7.92         7.92         7.92         7.92      ]\n",
      "  [  7.92         7.92         7.92         7.92      ]\n",
      "  [  7.92         7.92         7.92         7.92      ]\n",
      "  [  7.92         7.92         7.92         7.92      ]\n",
      "  [  7.92         7.92         7.92         7.92      ]\n",
      "  [  7.92         7.92         7.92         7.92      ]\n",
      "  [  7.91999989   7.91999999   7.92         7.92      ]\n",
      "  [  7.92       107.91999484   7.91998597   7.92      ]]\n",
      "\n",
      " [[  8.91         8.91         8.91         8.91      ]\n",
      "  [  8.91         8.91         8.91         8.91      ]\n",
      "  [  8.91         8.91         8.91         8.91      ]\n",
      "  [  8.91         8.91         8.91         8.91      ]\n",
      "  [  8.91         8.91         8.91         8.91      ]\n",
      "  [  8.91         8.91         8.91         8.91      ]\n",
      "  [  8.91         8.91         8.91         8.91      ]\n",
      "  [  8.91         8.91         8.91         8.91      ]\n",
      "  [  8.90999957   8.90999987   8.90999987 108.90994212]\n",
      "  [  0.           0.           0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "a = 0.7\n",
    "y = 0.99\n",
    "\n",
    "class Environment2:\n",
    "    def __init__(self):\n",
    "        # Define the grid size and initialize the field matrix\n",
    "        self.field = np.zeros((10, 10), dtype=str)  # 10x10 grid initialized as empty\n",
    "        self.field[:] = ' '  # Fill with empty spaces\n",
    "        self.goal_state = [9, 9]  # Goal at bottom-right corner\n",
    "        self.max_steps = 1000  # Maximum steps per episode\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        \"\"\"State transition function based on action (up, down, left, right).\"\"\"\n",
    "        # Apply the action, updating the agent's row or column position\n",
    "        if action == 0 and state[0] > 0: # linhas\n",
    "            state[0] -= 1 # linhas - 1\n",
    "        elif action == 1 and state[0] < len(self.field[0]) - 1: # linhas\n",
    "            state[0] += 1 # linhas + 1\n",
    "        elif action == 2 and state[1] > 0:\n",
    "            state[1] -= 1 # colunas - 1\n",
    "        elif action == 3 and state[1] < len(self.field[0]) - 1:\n",
    "            state[1] += 1 # colunas + 1\n",
    "        return state\n",
    "    \n",
    "    def transfake(self, state, action):\n",
    "        \"\"\"State transition function based on action (up, down, left, right).\"\"\"\n",
    "        # Apply the action, updating the agent's row or column position\n",
    "        if action == 0 and state[0] > 0: # linhas\n",
    "            return [state[0]-1,state[1]]\n",
    "        elif action == 1 and state[0] < len(self.field[0]) - 1: # linhas\n",
    "            return [state[0] +1,state[1]]\n",
    "        elif action == 2 and state[1] > 0:\n",
    "            return [state[0],state[1] -1]\n",
    "        elif action == 3 and state[1] < len(self.field[0]) - 1:\n",
    "            return [state[0] ,state[1] +1]\n",
    "        return state\n",
    "\n",
    "    def reward(self, state):\n",
    "        \"\"\"Reward function: 100 if goal is reached, otherwise 0.\"\"\"\n",
    "        return 100 if state == self.goal_state else 0\n",
    "\n",
    "    def print_matrix(self, agent_state):\n",
    "        \"\"\"Print the field with 'O' for the agent and 'X' for the goal.\"\"\"\n",
    "        # Create a copy of the field to modify for printing\n",
    "        display_field = self.field.copy()\n",
    "        # Mark the agent's and goal's positions\n",
    "        display_field[agent_state[0], agent_state[1]] = 'O'\n",
    "        display_field[self.goal_state[0], self.goal_state[1]] = 'X'\n",
    "        \n",
    "        # Print each row of the matrix\n",
    "        for row in display_field:\n",
    "            print(' '.join(row))\n",
    "        print(\"\\n\" + \"-\" * 20)  # Divider between prints\n",
    "\n",
    "class Agent2:\n",
    "    def __init__(self, environment):\n",
    "        self.q = np.zeros((10, 10, 4))\n",
    "        self.env = environment\n",
    "        self.current_state = [0, 0]  # Starting position at top-left corner\n",
    "        self.actions = [0,1,2,3]\n",
    "\n",
    "    def choose_random_action(self):\n",
    "        \"\"\"Randomly select an action.\"\"\"\n",
    "        return random.choice(self.actions)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent to the starting position.\"\"\"\n",
    "        self.current_state = [0, 0]\n",
    "\n",
    "    def run_episode(self):\n",
    "        \"\"\"Run an episode until goal is reached or max steps are exceeded.\"\"\"\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        for i in range(self.env.max_steps):\n",
    "            action = self.choose_random_action()\n",
    "            new_state = self.env.transfake(self.current_state, action)\n",
    "            total_reward += self.env.reward(new_state)\n",
    "            steps += 1\n",
    "\n",
    "            novo_q = Q(self,new_state,action)\n",
    "            \n",
    "            self.q[self.current_state[0]][self.current_state[1]][action] = novo_q[0]\n",
    "            self.current_state = new_state\n",
    "\n",
    "            # Print the matrix every 100 iterations\n",
    "            #if i % 10 == 0:\n",
    "            #print(f\"Iteration {i}\")\n",
    "            #self.env.print_matrix(self.current_state)\n",
    "\n",
    "            # Check if goal state is reached\n",
    "            if self.current_state == self.env.goal_state:\n",
    "                print(\"Reached the goal!\")\n",
    "                break\n",
    "\n",
    "        q = self.q\n",
    "        self.reset()\n",
    "        return total_reward, steps, q\n",
    "\n",
    "\n",
    "\n",
    "def Q(agente,novo_estado,acao):\n",
    "    return (1-a)*np.array(agente.q[agente.current_state[0]][agente.current_state[1]][acao]) + a*recompensa(agente,novo_estado)\n",
    "\n",
    "def recompensa(agente,novo_estado):\n",
    "\n",
    "    if novo_estado == agente.env.goal_state:\n",
    "        k = 100\n",
    "    else:\n",
    "        k = 0\n",
    "    \n",
    "    max = 1000\n",
    "    melhor = [0,0]\n",
    "    \n",
    "    for i in agente.actions:\n",
    "        #agente2 = agente\n",
    "        if np.sum(np.array(agente.env.goal_state) - np.array(agente.env.transfake(novo_estado,i))) < max and np.array_equal(np.array(agente.env.transfake(novo_estado,i)),agente.current_state):\n",
    "            max = np.sum(np.array(agente.env.goal_state) - np.array(agente.env.transfake(novo_estado,i)))\n",
    "            melhor = np.array(agente.env.transfake(novo_estado,i))\n",
    "\n",
    "    return k + y*melhor\n",
    "\n",
    "# Run simulation for 30 episodes\n",
    "def simulate2(agent, num_episodes=30):\n",
    "    rewards, steps_list, q_lista = [], [], []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        reward, steps, q = agent.run_episode()\n",
    "        rewards.append(reward)\n",
    "        steps_list.append(steps)\n",
    "        q_lista.append(q)\n",
    "\n",
    "    #print(f\"Reward: {rewards}\")\n",
    "    #print(f\"Steps: {steps_list}\")\n",
    "    #print(f\"Q: {q_lista}\")\n",
    "\n",
    "    print(\"OLAAAAAAAAAAAAAAAAAAADSADSJFHUBVNCJDHBGVVHUNDHERGFHBNDJEHFRUD\")\n",
    "\n",
    "    print(np.mean(q_lista, axis=0))\n",
    "\n",
    "    return rewards, steps_list, q_lista\n",
    "\n",
    "ambiente = Environment2()\n",
    "agente = Agent2(ambiente)\n",
    "\n",
    "rewards, steps_list, lista_q = simulate2(agente)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
